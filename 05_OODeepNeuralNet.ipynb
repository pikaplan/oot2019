{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"05_OODeepNeuralNet.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"3b_ugs_JOq2q","colab_type":"text"},"source":["# Object Oriented Deep Neural Networks\n","\n","<img src=\"https://images.squarespace-cdn.com/content/v1/565272dee4b02fdfadbb3d38/1521403348811-P4RMPTMIIVYZ84AHJKKL/ke17ZwdGBToddI8pDm48kMdFogk4zjxAyRDEXPTJab0UqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKcSBJjw6RvutB-lteUIUBBCmTHGjj7_rH39SC6UGXex2iIDOf1Tmg0PX5LsnDS6ZM3/Blank+Diagram+-+Page+1.png?format=750w\" width=\"480\" border=\"1\"/>\n","\n","\n","The effort of building, expanding and maintaining complex deep neural network architectures can be alleviated using principles of **Object Oriented Programming (OOP)**. This approach belongs to the domain of **Machine Learning Engineering** combines the best of at least two worlds: Software Engineering + Machine Learning.\n","\n","*(Objective: Illustrate reusability and the implementation of a complex model with readable code. Time: 10 mins)*"]},{"cell_type":"code","metadata":{"id":"4jb45kruiR2d","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive/')\n","\n","import sys\n","sys.path.append('/content/gdrive/My Drive/Colab Notebooks/OOT2019/lib')\n","\n","# Display the system path elements\n","for sFolder in sys.path:\n","  print(sFolder)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h20wpnMlQTu7","colab_type":"text"},"source":["Install ngrok"]},{"cell_type":"code","metadata":{"id":"hN2-Y-_YQR-D","colab_type":"code","colab":{}},"source":["! wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","! unzip -o ngrok-stable-linux-amd64.zip"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L2Rri7QhOgm_","colab_type":"text"},"source":["## Object oriented model declaration\n","\n","When declaring the layers of a neural network model there is a lot of repetition which suits to the **DRY (Don't Repeat Yourself) principle** of Object Oriented Programming. We can **encapsulate** the whole declaration inside an object for the model, cleaning up the main method of our program. Using **inheritance** we can make the basics of neural networks reusable and facilitate the declaration of deeper and more complex networks."]},{"cell_type":"markdown","metadata":{"id":"baYUprBIQYjw","colab_type":"text"},"source":["### Implementation Notes:\n","*   The logic for declaring the tensors of a fully connected layer in placed in the ancestor class `:NeuralNetwork`. \n","*   The descendand class `:DeeperNetwork ` overrides a virtual method ` CreateModel()` to create its custom architecture.\n","*   The weights of the fully connected layer are initialized randomly from a normal distribution with specified mean and standard deviation. Any values whose magnitude is more than 2 standard deviations from the mean are dropped and re-picked (truncated).\n","\n","### Theory: The choice for activation function\n","*   If we use sigmoidal activation functions in deeper networks we face the [Vanishing Gradients Problem](https://towardsdatascience.com/the-vanishing-gradient-problem-69bf08b15484). This is why rectifiers are preferred.\n","\n","     <img src=\"https://qph.fs.quoracdn.net/main-qimg-07bc0ec05532caf5ebe8b4c82d0f5ca3\" width=\"500\" border=\"1\"/>"]},{"cell_type":"code","metadata":{"id":"4L_bdPxmOhyf","colab_type":"code","colab":{}},"source":["# NEURAL NETWORK - Object Oriented\n","\n","import tensorflow.compat.v1 as tf\n","import math\n","\n","TENSORBOARD_FOLDER = \"/tmp/dnn\"\n","\n","#==================================================================================================\n","class NeuralNetwork(object):\n","    #------------------------------------------------------------------------------------\n","    def __init__(self):\n","        #........ |  Instance Attributes | ..............................................\n","        # // Collections that keep the neural network model parameters \\\\\n","        self.FCWeights = []\n","        self.FCBiases  = []\n","        #................................................................................\n","        \n","        # This is a call to virtual method that descendants override\n","        self.CreateModel()\n","    #------------------------------------------------------------------------------------\n","    def CreateModel(self):\n","        pass\n","    #------------------------------------------------------------------------------------\n","    def GetParameter(self, p_tShape, p_tInitializer=tf.initializers.constant(0.0), p_bIsBias=False):\n","      if p_bIsBias:\n","        sParamName = \"b\"\n","      else:\n","        sParamName = \"w\"\n","    \n","      tParam = tf.get_variable(sParamName, shape=p_tShape, initializer=p_tInitializer)\n","      return tParam\n","    #------------------------------------------------------------------------------------\n","    def FullyConnected(self, p_tInput, p_nNeuronCount, p_tActivationFunction=None):\n","      nLayerNum = len(self.FCWeights) + 1\n","      nInputNeurons = p_tInput.get_shape().as_list()[-1]\n","      \n","      with tf.variable_scope(\"FC%d\" % nLayerNum):\n","        tW = self.GetParameter([nInputNeurons, p_nNeuronCount], tf.initializers.truncated_normal(mean=0.0, stddev=math.sqrt(2/(nInputNeurons+p_nNeuronCount)))) \n","        tB = self.GetParameter([p_nNeuronCount], p_bIsBias=True ) \n","        tU = tf.matmul(p_tInput, tW) + tB\n","        \n","        if p_tActivationFunction is not None:\n","          # Activation function is a function reference that is passed to the method\n","          tA = p_tActivationFunction(tU)\n","        else:\n","          tA = tU\n","      \n","        self.FCWeights.append(tW)\n","        self.FCBiases.append(tB)\n","      \n","      return tA\n","    #------------------------------------------------------------------------------------\n","    \n","#==================================================================================================\n","\n","\n","\n","\n","    \n","#==================================================================================================  \n","class DeeperNetwork(NeuralNetwork):  \n","    #------------------------------------------------------------------------------------\n","    def __init__(self, p_nFeatures=[128,256,512,10]):\n","        #........ |  Instance Attributes | ..............................................\n","        # // Network input and output tensors \\\\\n","        self.Input      = None\n","        self.Logits     = None\n","        self.Prediction = None\n","        \n","        # // Architectural hyperparameters \\\\\n","        self.Features = p_nFeatures\n","        #................................................................................\n","        \n","        # Invoke the inherited logic from ancestor :NeuralNetwork\n","        super(DeeperNetwork, self).__init__()\n","    #------------------------------------------------------------------------------------\n","    def CreateModel(self):\n","      with tf.variable_scope(\"NeuralNet\"):\n","        with tf.variable_scope(\"Input\"):\n","          self.Input = tf.placeholder(tf.uint8, shape=(100,768,3))\n","          tInputNormalized = tf.cast(tf.cast(self.Input, tf.float32) / 255.0, tf.float32)\n","          tX = tInputNormalized\n","        \n","        nLayerIndex = 1\n","        with tf.variable_scope(\"L%d\" % nLayerIndex):\n","          tA = tf.nn.relu(self.FullyConnected(tX, self.Features[0]))\n","        \n","        # Using same feature depth inside the context of a module (layers 2-9 and 10-17)\n","        with tf.variable_scope(\"Module1\"):\n","            for nLayerIndex in range(2,10):\n","              with tf.variable_scope(\"L%d\" % nLayerIndex):\n","                tA = tf.nn.relu(self.FullyConnected(tA, self.Features[1]))\n","                          \n","        with tf.variable_scope(\"Module2\"):\n","            for nLayerIndex in range(10, 18):\n","              with tf.variable_scope(\"L%d\" % nLayerIndex):              \n","                tA = tf.nn.relu(self.FullyConnected(tA, self.Features[2]))\n","        \n","        with tf.variable_scope(\"Classifier\"):\n","          self.Logits     = self.FullyConnected(tA, self.Features[-1])\n","          self.Prediction = tf.nn.softmax(self.Logits)\n","    #------------------------------------------------------------------------------------\n","    \n","#==================================================================================================  \n","\n","  \n","  \n","  \n","\n","\n","#------------------------------------------------------------------------------------\n","def Main():\n","  oGraph = tf.Graph()\n","  with oGraph.as_default():\n","    oNet = DeeperNetwork([128,256,512,10])\n","\n","    with tf.Session() as oSession:\n","      assert oSession.graph == oGraph, \"The current session is using the default graph\"\n","      oSession.run(tf.initializers.global_variables())\n","\n","\n","      oWriter = tf.summary.FileWriter(TENSORBOARD_FOLDER, graph=oSession.graph, flush_secs=20)\n","      oWriter.flush()\n","      print(\"Graph exported to %s\" % TENSORBOARD_FOLDER)\n","#------------------------------------------------------------------------------------\n","\n","\n","\n","    \n","if __name__ == '__main__':\n","  Main()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nSfXPR74Oh9e","colab_type":"text"},"source":["Display the graph in Tensorboard"]},{"cell_type":"code","metadata":{"id":"SIYDmrWvOYwK","colab_type":"code","colab":{}},"source":["# kill all running ngrok instances\n","!pkill -f ngrok\n","!pkill -f tensorboard\n","\n","# Execute tensorboard\n","LOG_DIR = '/tmp/dnn/'\n","get_ipython().system_raw('tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'.format(TENSORBOARD_FOLDER))\n","\n","# execute ngrok\n","get_ipython().system_raw('./ngrok http 6006 &')\n","\n","# Do the tunneling\n","! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n","    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ezLMNL4miVMJ","colab_type":"text"},"source":["We move the ancestor class for neural networks `:NeuralNetwork ` in the **oot.nn** namespace to make it reusable. Our source code becomes even more simplified, keeping only the custom implementation. We improve the graph by adding extra hierarchy in the variable scopes"]},{"cell_type":"code","metadata":{"id":"S-jqKisbidJh","colab_type":"code","colab":{}},"source":["# NEURAL NETWORK - Object Oriented\n","\n","import tensorflow.compat.v1 as tf\n","import math\n","from ootf.nn import NeuralNetwork\n","\n","\n","TENSORBOARD_FOLDER = \"/tmp/dnn\"\n","\n","    \n","#==================================================================================================  \n","class DeeperNetwork(NeuralNetwork):  \n","    #------------------------------------------------------------------------------------\n","    def __init__(self, p_nFeatures=[128,256,512,10]):\n","        #........ |  Instance Attributes | ..............................................\n","        # // Network input and output tensors \\\\\n","        self.Input      = None\n","        self.Logits     = None\n","        self.Prediction = None\n","        \n","        # // Architectural hyperparameters \\\\\n","        self.Features = p_nFeatures\n","        #................................................................................\n","        \n","        # Invoke the inherited logic from ancestor :NeuralNetwork\n","        super(DeeperNetwork, self).__init__()\n","    #------------------------------------------------------------------------------------\n","    def CreateModel(self):\n","      with tf.variable_scope(\"NeuralNet\"):\n","        with tf.variable_scope(\"Input\"):\n","          self.Input = tf.placeholder(tf.uint8, shape=(100,768,3))\n","          tInputNormalized = tf.cast(tf.cast(self.Input, tf.float32) / 255.0, tf.float32)\n","          tX = tInputNormalized\n","        \n","        nLayerIndex = 1\n","        with tf.variable_scope(\"L%d\" % nLayerIndex):\n","          tA = tf.nn.relu(self.FullyConnected(tX, self.Features[0]))\n","        \n","        # Using same feature depth inside the context of a module (layers 2-9 and 10-17)\n","        with tf.variable_scope(\"Module1\"):\n","            for nLayerIndex in range(2,10):\n","              with tf.variable_scope(\"L%d\" % nLayerIndex):\n","                tA = tf.nn.relu(self.FullyConnected(tA, self.Features[1]))\n","                          \n","        with tf.variable_scope(\"Module2\"):\n","            for nLayerIndex in range(10, 18):\n","              with tf.variable_scope(\"L%d\" % nLayerIndex):              \n","                tA = tf.nn.relu(self.FullyConnected(tA, self.Features[2]))\n","        \n","        with tf.variable_scope(\"Classifier\"):\n","          self.Logits     = self.FullyConnected(tA, self.Features[-1])\n","          self.Prediction = tf.nn.softmax(self.Logits)\n","    #------------------------------------------------------------------------------------\n","    \n","#==================================================================================================  \n","\n","\n","#------------------------------------------------------------------------------------\n","def Main():\n","  oGraph = tf.Graph()\n","  with oGraph.as_default():\n","    oNet = DeeperNetwork([128,256,512,10])\n","\n","    with tf.Session() as oSession:\n","      assert oSession.graph == oGraph, \"The current session is using the default graph\"\n","      oSession.run(tf.initializers.global_variables())\n","\n","\n","      oWriter = tf.summary.FileWriter(TENSORBOARD_FOLDER, graph=oSession.graph, flush_secs=20)\n","      oWriter.flush()\n","      print(\"Graph exported to %s\" % TENSORBOARD_FOLDER)\n","#------------------------------------------------------------------------------------\n","\n","\n","\n","    \n","if __name__ == '__main__':\n","  Main()"],"execution_count":0,"outputs":[]}]}