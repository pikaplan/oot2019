{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"08_TrainResNetOnCIFAR10.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"SQeiiFDGupFD","colab_type":"text"},"source":["# How To Train Your ResNet\n","\n","<img src=\"https://raw.githubusercontent.com/zhanghang1989/ResNet-Matconvnet/master/figure/resnet_cifar.png\" width=\"640\" border=\"1\"/>\n","\n","At this point we have assembled the classes into a small framework that resides under the `/lib/ootf` folder. We are going to deploy a state-of-the-art CNN architecture, that combines increased accuracy with small memory footprint.\n","\n","`(Objective: Train a ResNet20 on CIFAR10 with ~90% accuracy, understand residual networks through the source code. Time: 40 mins)`"]},{"cell_type":"markdown","metadata":{"id":"taH7K9SuR9Og","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"HqwWASVtRE6D","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zrit0Y3QukGE","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive/')\n","print()\n","\n","# Check GPU\n","import tensorflow as tf\n","print(tf.test.gpu_device_name())\n","     \n","import sys\n","sys.path.append('/content/gdrive/My Drive/Colab Notebooks/OOT2019/lib')\n","\n","# Display the system path elements\n","for sFolder in sys.path:\n","  print(sFolder)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ehbF_jJmuqTJ","colab_type":"text"},"source":["We import the classes from the `ootf` package. Starting with the settings for the training process."]},{"cell_type":"code","metadata":{"id":"_jI4Tff2urN4","colab_type":"code","colab":{}},"source":["import numpy as np\n","import tensorflow as tf\n","from datetime import datetime\n","from ootf.tfcifar10 import TFDataSetCifar10\n","from ootf.resnet import ResNet\n","from ootf.base import ModelState\n","\n","\n","#  ........... Settings ...........\n","RANDOM_SEED         = 2019\n","DATA_FOLDER         = \"/content/gdrive/My Drive/Colab Notebooks/OOT2019/data/tfcifar10\"\n","TRAINING_INTERVAL   = 250\n","VALIDATION_INTERVAL = 1000\n","SAVE_INTERVAL       = 2000\n","\n","# (He,2015) We start with a learningrate of 0.1, divide it by 10 at 32k and 48k iterations, and terminate training at 64k iterations\n","INITIAL_LEARNING_RATE = 0.1\n","INITIAL_MOMENTUM = 0.9\n","LR_SCHEDULE_STEPS_ORIGINAL = [32000,48000,64000]\n","\n","\n","# Makes the random initialization reproducible\n","tf.reset_default_graph() \n","tf.set_random_seed(RANDOM_SEED)\n","np.random.seed(RANDOM_SEED)\n","print(\"Random seed set to %d\" % RANDOM_SEED)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YajAUlbYvODp","colab_type":"text"},"source":["The data feed thread will load TFRecords from the files that were created in `/data/tfcifar10`"]},{"cell_type":"code","metadata":{"id":"ccGQgl3zvK7l","colab_type":"code","colab":{}},"source":["# ........... Loads data / Creates the data feed ...........\n","oDataFeed = TFDataSetCifar10(DATA_FOLDER)\n","LR_SCHEDULE_STEPS = LR_SCHEDULE_STEPS_ORIGINAL\n","print(\" |_ Learning rate schedule:%s. Batch sizes, training: %d testing: %d.\" % (LR_SCHEDULE_STEPS, oDataFeed.TrainBatchSize, oDataFeed.TestBatchSize))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9tZaBQM_8Qta","colab_type":"text"},"source":["## Non-standard settings \n","We increase the training batch size to x4 compared with the standard size. Please keep in mind that such change require hyperparameter tunning, to achieve the optimum training result."]},{"cell_type":"code","metadata":{"id":"CBJMy12v8QdC","colab_type":"code","colab":{}},"source":["from ootf.nn import NeuralNetwork\n","\n","# We are using x4 the default batch size\n","oDataFeed.TrainBatchSize *= 4\n","oDataFeed.TestBatchSize *= 5\n","\n","# It will need half an hour on GPU to achieve 89.7% accuracy \n","LR_SCHEDULE_STEPS = [6000, 7000, 8000] \n","\n","print(\" |_ Learning rate schedule:%s. Batch sizes, training: %d testing: %d.\" % (LR_SCHEDULE_STEPS, oDataFeed.TrainBatchSize, oDataFeed.TestBatchSize))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S6uhs7RQurgq","colab_type":"text"},"source":["## Residual CNN architecture\n","\n","We create the ResNet model. The second parameter in the constructor defines the type of ResNet. For ResNet20 we have 3 stacks of 3 modules that is 3\\*3\\*2+2=20. For ResNet32 we have 3 stacks of 5 modules 3\\*5\\*2+2=32. Please refer to the original paper [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) for other architectures. \n","\n","<img src=\"https://raw.githubusercontent.com/torch/torch.github.io/master/blog/_posts/images/resnets_modelvariants.png\" width=\"600\" border=\"1\"/>\n","\n","### Implementation Notes\n","*   The `:ResidualModule` implements the original(reference) paper\n","*   When downsampling is performed with stride 2 the skip connection is implemented in method `ResidualConnection()` as an average pooling layer. It can be replaced with a learnable convolutional layer.\n"]},{"cell_type":"code","metadata":{"id":"-PQmMBN8ur9R","colab_type":"code","colab":{}},"source":["import numpy as np\n","from tensorflow.python import control_flow_ops\n","\n","from ootf.cnn import ConvolutionalNeuralNetwork\n","from ootf.base import Evaluator\n","from ootf.nn import NeuralNetwork\n","\n","\n","# =======================================================================================================================\n","class ResidualModule(object):\n","    # --------------------------------------------------------------------------------------------------------\n","    def __init__(self, p_oParent, p_nFeatures, p_nStrideOnInput):\n","        #......................... |  Instance Attributes | .............................\n","        # // Aggregates \\\\ \n","        self.Parent         = p_oParent\n","        \n","        # // Settings \\\\\n","        self.Features       = p_nFeatures\n","        self.StrideOnInput  = p_nStrideOnInput\n","        \n","        # // Tensors \\\\\n","        self.Input          = None\n","        self.InputFeatures  = None\n","        self.Output         = None\n","        #................................................................................\n","    # --------------------------------------------------------------------------------------------------------\n","    def ResidualConnection(self, p_tModuleInput):\n","        with tf.variable_scope(\"RESIDUAL\"):\n","          # Spatial downsampling in the first convolutional layer of the module and the skip connection\n","          if (self.StrideOnInput > 1):\n","              tSpatialDownsampling = self.Parent.AveragePool(p_tModuleInput, (2,2), (2,2), p_bIsPadding=False)\n","          else:\n","              tSpatialDownsampling = p_tModuleInput\n","                  \n","          # Pad features if dimensionality of features increases \n","          if (self.InputFeatures != self.Features):\n","              tResidual = self.Parent.PadFeatures(tSpatialDownsampling, self.Features, p_sName=\"pad_skip\")\n","          else:\n","              tResidual = tf.identity(tSpatialDownsampling, \"skip\")\n","  \n","        return tResidual\n","    # --------------------------------------------------------------------------------------------------------\n","    def Function(self, p_tX):\n","        self.Input          = p_tX\n","        self.InputFeatures  = self.Input.get_shape().as_list()[3]\n","\n","        sModuleName = \"RES%d\" % (len(self.Parent.Modules) + 1)\n","        with tf.variable_scope(sModuleName):\n","            tResidual = self.ResidualConnection(p_tX)\n","            \n","            tA = self.Parent.Convolutional(p_tX, self.Features, (3,3), (self.StrideOnInput, self.StrideOnInput), p_nPadding=1, p_bHasBias=False, p_tInitializer=tf.initializers.he_uniform()) #HE INITIALIZATION\n","            tA = self.Parent.BatchNormalization(tA)\n","            tA = tf.nn.relu(tA)\n","            \n","            tA = self.Parent.Convolutional(tA, self.Features, (3,3), (1,1), p_nPadding=1, p_bHasBias=False, p_tInitializer=tf.initializers.he_uniform())\n","            tA = self.Parent.BatchNormalization(tA)\n","            \n","            tY = tf.nn.relu(tA + tResidual)\n","                \n","            self.Output = tY    \n","        return tY\n","    # --------------------------------------------------------------------------------------------------------      \n","# =======================================================================================================================\n","\n","\n","\n","\n","\n","   \n","#==================================================================================================  \n","class ResNet(ConvolutionalNeuralNetwork):  \n","    #------------------------------------------------------------------------------------\n","    def __init__(self, p_nFeatures=[(32,32,3),16,32,64,10], p_nStackSetup=[5,5,5], p_oDataFeed=None):\n","        #......................... |  Instance Attributes | .............................\n","        # // Composite object collections \\\\\n","        self.Modules = []\n","        \n","        # // Aggregates  \\\\\n","        self.DataFeed = p_oDataFeed\n","                \n","        # // Architectural Hyperparameters \\\\\n","        self.StackSetup = p_nStackSetup\n","        self.DownSampling = [False, True, True, True]\n","\n","        # // Learning Hyperparameters \\\\\n","        #self.Momentum     = 0.9\n","        self.WeightDecay  = 1e-4\n","        self.DropOutRate  = 0.5\n","\n","        # // Tensors \\\\\n","        self.Input            = None\n","        self.Targets          = None\n","        self.TargetsOneHot    = None\n","        \n","        self.Logits           = None\n","        self.Prediction       = None\n","        self.PredictedClass   = None\n","        self.Correct          = None\n","        self.Accuracy         = None\n","        \n","        self.WeightDecayCost  = None\n","        self.CCECost          = None\n","        self.CostFunction     = None\n","        #................................................................................\n","        \n","        # Invoke the inherited logic from ancestor :NeuralNetwork\n","        super(ResNet, self).__init__(p_nFeatures)\n","    # --------------------------------------------------------------------------------------------------------\n","    def __getModuleStrides(self, p_nStackIndex):\n","        nModuleCount = self.StackSetup[p_nStackIndex]\n","    \n","        if  self.DownSampling[p_nStackIndex]: \n","            nStrides = [2]\n","        else:\n","            nStrides = [1]\n","        nStrides = nStrides + [1]*(nModuleCount -1)\n","        \n","        return nStrides        \n","    # --------------------------------------------------------------------------------------------------------\n","    def CreateInput(self):\n","        assert self.DataFeed is not None, \"This model works with a TFRecords data feed\"\n","      \n","        tTrainImageBatch, tTrainLabelBatch = self.DataFeed.TrainingBatches()\n","        tTestImageBatch, tTestLabelBatch = self.DataFeed.TestingBatches()\n","        \n","        tImages, tLabels = control_flow_ops.cond(self.IsTraining,\n","            lambda: (tTrainImageBatch, tTrainLabelBatch),\n","            lambda: (tTestImageBatch, tTestLabelBatch))\n","    \n","        self.Input = tImages\n","        self.Targets   = tLabels   \n","        \n","        return self.Input, self.Targets \n","    def Feed(self, p_nBatchFeatures, p_nBatchTargets=None, p_nLearningRate=None, p_bIsTraining=False):\n","      oDict = dict()\n","      \n","      oDict[self.Input]      = p_nBatchFeatures\n","      oDict[self.IsTraining] = p_bIsTraining\n","      \n","      if p_bIsTraining:\n","        oDict[self.Targets]      = p_nBatchTargets\n","        oDict[self.LearningRate] = p_nLearningRate\n","        oDict[self.DropOutKeepProb] = self.DropOutRate\n","      else:\n","        # When the model is trained the neurons are not dropped out\n","        oDict[self.DropOutKeepProb] = 1.0\n","        \n","      return oDict\n","    # --------------------------------------------------------------------------------------------------------\n","    def Predict(self, p_oSession, p_oSubSet):   \n","      nPredictedClasses = np.zeros(p_oSubSet.Labels.shape, np.uint32)\n","      for nRange, nSamples, nLabels, _ in p_oSubSet:\n","        nPrediction = p_oSession.run(self.Prediction, feed_dict=self.Feed(nSamples, nLabels))\n","        nPredictedClasses[nRange] = np.argmax(nPrediction, axis=1).astype(np.uint32)\n","      \n","      return nPredictedClasses\n","    # --------------------------------------------------------------------------------------------------------\n","    def Evaluate(self, p_oDataSubSet=None, p_nBatchSize=1000, p_oProcess=None, p_oSession=None):\n","        assert self.DataFeed is not None, \"This model works with a TFRecords data feed\"\n","      \n","        n_val_samples = 10000\n","        val_batch_size = self.DataFeed.TestBatchSize\n","        \n","        n_val_batch = int(  np.ceil(n_val_samples / val_batch_size) )\n","        \n","        #val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n","        val_labels = np.zeros((n_val_samples), dtype=np.int64)\n","        \n","        pred_labels = np.zeros((n_val_samples), dtype=np.int64)\n","        val_losses = []\n","        for i in range(n_val_batch):\n","            fetches = [self.Prediction, self.Targets, self.CostFunction]\n","            fetches.append(self.PredictedClass)\n","            \n","            if p_oDataSubSet is None:\n","                oFeedDict = {self.IsTraining: False}\n","            else:\n","                oFeedDict = dict()\n","                oFeedDict[self.Input]       = p_oDataSubSet.Patterns[i * val_batch_size:(i + 1) * val_batch_size,...]\n","                oFeedDict[self.Targets]     = p_oDataSubSet.Labels[i * val_batch_size:(i + 1) * val_batch_size,...]\n","                oFeedDict[self.IsTraining]  = False\n","                \n","            if p_oSession is not None:\n","              session_outputs = p_oSession.run(fetches, oFeedDict)\n","            else:   \n","              session_outputs = self.Session.run(fetches, oFeedDict)\n","            \n","            pred_labels[i * val_batch_size:(i + 1) * val_batch_size] = session_outputs[3]\n","            val_labels[i * val_batch_size:(i + 1) * val_batch_size] = session_outputs[1]\n","            val_losses.append(session_outputs[2])\n","        \n","        oEval = Evaluator(val_labels, pred_labels)\n","        # print(nAvgBatchesAccuracy)\n","        if p_oProcess is None:\n","            print(\"Accuracy:\", oEval.Accuracy)\n","        else:\n","            p_oProcess.Print(\"Accuracy:\", oEval.Accuracy)\n","        print(oEval.ConfusionMatrix)\n","                            \n","        val_loss     = float(np.mean(np.asarray(val_losses)))    \n","        val_accuracy = oEval.Accuracy\n","        \n","        return val_loss, val_accuracy\n","    # --------------------------------------------------------------------------------------------------------\n","    def CreateModel(self):\n","      nClassCount = self.Features[-1]\n","        \n","      with tf.variable_scope(\"NeuralNet\"):\n","        #tInput, tTargets = \n","        self.CreateInput()\n","        \n","        with tf.variable_scope(\"Targets\"):\n","          self.TargetsOneHot = tf.one_hot(self.Targets, depth=nClassCount, dtype=tf.float32)\n","\n","        with tf.variable_scope(\"Stem\"):\n","            tA = self.Convolutional(self.Input, self.Features[1], (3,3), (1,1), p_bIsPadding=True, p_bHasBias=False, p_tInitializer=tf.initializers.he_uniform())\n","            tA = self.BatchNormalization(tA)\n","            tA = tf.nn.relu(tA)\n","\n","        # Creates the stacks of residual modules\n","        nStackCount = len(self.StackSetup)\n","        for nStackIndex in range(0, nStackCount):\n","            nModuleStrides = self.__getModuleStrides(nStackIndex)\n","            \n","            sStackName = \"Stack%d\" % (nStackIndex + 1)\n","            with tf.variable_scope(sStackName):\n","                print(\"  [%s]\" % sStackName)              \n","                for nModuleIndex, nModuleInputStride in enumerate(nModuleStrides):\n","                    oResBlock = ResidualModule(self, self.Features[nStackIndex + 1], nModuleInputStride)\n","                    tA = oResBlock.Function(tA)\n","                    \n","                    self.Modules.append(oResBlock)\n","                    print(\" |_ Res%d\" % (nModuleIndex+1), oResBlock.Input, oResBlock.Output)                                      \n","                  \n","        # For each output feature,  averages the values in its spatial activation table. \n","        # This results feature activation vector for each image\n","        tA = self.GlobalAveragePooling(tA)\n","        \n","        # Softmax layer (classifier)\n","        self.Logits     = self.FullyConnected(tA, nClassCount)\n","        self.Prediction = tf.nn.softmax(self.Logits)       \n","        \n","        # Predictions and accuracy\n","        with tf.variable_scope(\"Predictions\"):\n","            self.PredictedClass     = tf.argmax(self.Prediction , 1, output_type=tf.int32)\n","            self.Correct            = tf.equal(self.PredictedClass, tf.cast(self.Targets, tf.int32))\n","            self.Accuracy           = tf.reduce_mean(tf.cast(self.Correct , tf.float32), name='accuracy')\n","        \n","      # Prepare cost function tensors for training\n","      self.DefineCostFunction()\n","    # --------------------------------------------------------------------------------------------------------\n","    def DefineCostFunction(self):\n","        with tf.variable_scope(\"Cost\"):\n","            # Multiclass categorical cross entropy (CCE) loss\n","            tLoss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.Logits, labels=self.TargetsOneHot)\n","            self.CCECost  = tf.reduce_mean(tLoss, name=\"cce\")\n","            \n","            # L2 weight decay regularization\n","            tL2LossesConv = [tf.nn.l2_loss(tKernel) for tKernel in self.ConvWeights]\n","            tL2LossesFC   = [tf.nn.l2_loss(tWeight) for tWeight in self.FCWeights]\n","            tL2LossesAll  = tL2LossesConv + tL2LossesFC\n","            tWeightDecay = tf.constant(self.WeightDecay, tf.float32, name=\"weight_decay\")\n","            self.WeightDecayCost = tf.multiply(tWeightDecay, tf.add_n(tL2LossesAll), name=\"l2\")\n","\n","            # Total cost\n","            self.CostFunction = tf.identity(self.CCECost + self.WeightDecayCost, \"total_cost\")\n","    # --------------------------------------------------------------------------------------------------------\n","#==================================================================================================  \n","\n","\n","\n","# ...................... Creates the model ......................\n","sModelName = \"ResNet20\"\n","\n","print(\"\\n[>] Creating %s model ...\" % sModelName)\n","\n","#(He,2015) Reported accuracy for ResNet20 in Table 6:91.25%               \n","oModel = ResNet([(32,32,3),16,32,64,10], [3,3,3], p_oDataFeed=oDataFeed)                    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"piodnDkVxbWA","colab_type":"text"},"source":["## Create a custom training operation\n","Tensorflow comes with easy to use optimizers that can create gradient tensors, through automatic derivation. The easiest is to call the `.minimize()` method on an optimizer. The example bellow illustrates how to create a custom training operation, by having the gradient tensors available, that is useful for more advanced optimization methods. "]},{"cell_type":"code","metadata":{"id":"JnFzH0YDvQhj","colab_type":"code","colab":{}},"source":["# ...................... Trains the model ......................        \n","# Creates the training operation\n","print(\"\\n[>] Creating training operation ...\")        \n","oUpdateOperations = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n","with tf.control_dependencies(oUpdateOperations):\n","    with tf.variable_scope(\"CustomOptimizer\"):\n","        # Gets the gradients for the cost function w.r.t. the trainable model parameters\n","        tModelParameters = tf.trainable_variables()\n","        tGradients       = tf.gradients(oModel.CostFunction, tModelParameters, name='gradients')\n","        print(\" |__ A total of %d trainable model parameters have %d gradients.\" % (len(tModelParameters), len(tGradients)))\n","        print(\"     |__ Convolutional Kernels:%d\" % len(oModel.ConvWeights))\n","        print(\"     |__ Convolutional Biases:%d\" % len(oModel.ConvBiases))\n","        print(\"     |__ Fully Connected Weights:%d\" % len(oModel.FCWeights))        \n","        print(\"     |__ Fully Connected Biases:%d\" % len(oModel.FCBiases))\n","        \n","        # Prioritize dependencies\n","        oUpdateOperations   = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n","        with tf.control_dependencies(oUpdateOperations):\n","            tOptimizer      = tf.train.MomentumOptimizer(oModel.LearningRate, INITIAL_MOMENTUM)\n","            tBackPropagate  = tOptimizer.apply_gradients(zip(tGradients, tModelParameters), global_step=oModel.GlobalStep)\n","            with tf.control_dependencies([tBackPropagate]):\n","                tTrainOp = tf.no_op(name='train_op')\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lzMgMH7_usRp","colab_type":"text"},"source":["We initialize the session and create some helper objects."]},{"cell_type":"code","metadata":{"id":"SYwlAUfcusYj","colab_type":"code","colab":{}},"source":["# Initializes the session and its variables\n","print(\"\\n[>] Initializing...\")\n","oSession = tf.Session()\n","oSession.run(tf.global_variables_initializer())\n","\n","# Creates helper objects\n","oState = ModelState(oSession, \"/content/gdrive/My Drive/Colab Notebooks/OOT2019/models/%s\" % sModelName)              \n","  \n","print(\" |__ GPU:\", tf.test.gpu_device_name())    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mU2RCusZy15L","colab_type":"text"},"source":["## Reproducible random weight initialization\n","\n","We can run this unit test to ensure that setting the random seed in Tensorflow results in reproducible initial weights.  Nevertheless the training process in the GPU is not deterministic due to the low level implementation of the 2D convolution.\n","\n","\n","No exceptions should be raised by the following test."]},{"cell_type":"code","metadata":{"id":"P6pymdMfy1ir","colab_type":"code","colab":{}},"source":["# --------------------------------------------------------------------------------------------------------  \n","def testReproducibility(p_oModel, p_oSession):\n","  tW_CONV19 = p_oModel.ConvWeights[-1]\n","  tW_FC1 = p_oModel.FCWeights[-1]\n","  \n","  nW_CONV19 = tW_CONV19.eval(p_oSession)\n","  nW_FC1 = tW_FC1.eval(p_oSession)\n","  \n","  print(\"[%s] Initial Weights: Mean=%.6f Std=%.6f\" % (tW_CONV19.name, np.round(np.mean(nW_CONV19), 6), np.round(np.std(nW_CONV19), 6)) )\n","  print(\"[%s] Initial Weights: Mean=%.6f Std=%.6f\" % (tW_FC1.name, np.round(np.mean(nW_FC1), 6), np.round(np.std(nW_FC1), 6)) )\n","  \n","  assert tW_CONV19.name == \"NeuralNet/Stack3/RES9/CONV19/w:0\", \"Architecture is not ResNet20\"\n","  assert tW_FC1.name == \"NeuralNet/FC1/w:0\", \"Architecture is not ResNet20\"  \n","  \n","  assert (np.round(np.mean(nW_CONV19), 6) - 0.000051) <= 1e-5, \"CONV19 kernel initial values have different mean than expected\"\n","  assert (np.round(np.std(nW_CONV19), 6) - 0.058901) <= 1e-5, \"CONV19 kernel initial values have different std than expecteds\"\n","\n","  assert (np.round(np.mean(nW_FC1), 6) - 0.000182) <= 1e-5, \"FC1 weight initial values have different mean than expected\"\n","  assert (np.round(np.std(nW_FC1), 6) - 0.163457) <= 1e-5, \"FC1 weight initial values have different std than expecteds\"\n","# --------------------------------------------------------------------------------------------------------    \n","  \n","\n","testReproducibility(oModel, oSession)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CVUwHlGpvcOf","colab_type":"text"},"source":["## Main training loop\n","The main training loop begins by starting the threads that feed data to an input queue. Running the training operation tensor requests the next minibatch of data automatically. Only the training flag and the learning rate are passed with a feed dictionary. At specific intervals i) more tensor values are retrieved, ii) the model is evaluated using the whole testing subset and iii) the model is saved."]},{"cell_type":"code","metadata":{"id":"Kl_uUPx_vcWm","colab_type":"code","colab":{}},"source":["print(\"[>] Started training at %s ...\" % datetime.now() )\n","      \n","# Starts the data feed threads\n","tCoordinator = tf.train.Coordinator()       \n","tThreads = tf.train.start_queue_runners(sess=oSession, coord=tCoordinator)\n","\n","# Training loop   \n","nMaxSteps  = LR_SCHEDULE_STEPS[-1]\n","nLR = INITIAL_LEARNING_RATE \n","nEpochIndex = 0\n","nLastValidationAccuracy = 0.0\n","for nStepNumber in range(1, nMaxSteps+1):\n","    bIsLastStep              = (nStepNumber == nMaxSteps) \n","    bIsTrainingPrintInterval = ((nStepNumber % TRAINING_INTERVAL) == 0) or bIsLastStep\n","    bIsValidationInterval    = ((nStepNumber % VALIDATION_INTERVAL) == 0) or bIsLastStep\n","    bIsSaveInterval          = ((nStepNumber % SAVE_INTERVAL) == 0) or bIsLastStep\n","    \n","    oTensorList = [tTrainOp, oModel.CostFunction]\n","    if bIsTrainingPrintInterval:\n","        oTensorList += [oModel.WeightDecayCost, oModel.Accuracy]\n","    nRunResult = oSession.run(oTensorList, feed_dict={oModel.IsTraining: True, oModel.LearningRate: nLR})\n","\n","    # Print training status\n","    if bIsTrainingPrintInterval:\n","        nTrainingTotalLoss, nTrainingL2Loss, nTrainingAccuracy, = nRunResult[1:]\n","        print('%s [%d] Iteration %6d | LR=%.6f | ERR=%.6f (WD=%.6F) | ACC=%.4f' % (datetime.now(), nEpochIndex+1, nStepNumber, nLR\n","                                                , nTrainingTotalLoss, nTrainingL2Loss, nTrainingAccuracy))\n","\n","    # Validation on the whole testing set\n","    if bIsValidationInterval:\n","        print('[%d] Evaluating...' % (nEpochIndex+1))\n","        oModel.Evaluate(p_oSession=oSession)\n","        nEpochIndex += 1\n","        \n","    # Save the model state\n","    if bIsSaveInterval:\n","        print(\"[>] Saving after %s epochs\" % SAVE_INTERVAL)\n","        oState.Save()\n","    \n","    # Implements the learning rate schedule\n","    if nStepNumber in LR_SCHEDULE_STEPS[:-1]:\n","      nLR = nLR * 0.1\n","      print(\"[>] Changed learning rate to %.5f\" % nLR)\n","      oState.Save()\n","                \n","# Terminates the data feed threads gracefully\n","tCoordinator.request_stop()\n","tCoordinator.join(tThreads)  \n","\n","oSession.close()\n","print(\"[>] Finished training at %s ...\" % datetime.now() )"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DUZX8Z7kfbXk","colab_type":"text"},"source":["## ResNet inference"]},{"cell_type":"code","metadata":{"id":"2LhsAE8Tfbo8","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from ootf.base import DataSubSet\n","from ootf.cifar10 import DataSetCifar10\n","\n","import requests\n","from PIL import Image\n","from io import BytesIO\n","import joblib\n","\n","MODEL_STATE_NAME = \"/content/gdrive/My Drive/Colab Notebooks/OOT2019/models/ResNet20\" \n","oNet = oModel\n","\n","# ... False predictions ...\n","IMAGE1 = \"https://upload.wikimedia.org/wikipedia/commons/thumb/8/8b/Husky_on_San_Francisco_sidewalk.jpg/220px-Husky_on_San_Francisco_sidewalk.jpg\"\n","IMAGE2 = \"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b1/Hellenic_Hound_aka_Hellinikos_Ichnilatis.jpg/220px-Hellenic_Hound_aka_Hellinikos_Ichnilatis.jpg\"\n","\n","# ... Correct predictions ...\n","IMAGE3 = \"https://vignette.wikia.nocookie.net/dog-breed4080/images/0/0a/GREATER-SWISS-MOUNTAIN-DOG.jpg/revision/latest/scale-to-width-down/230?cb=20171221134620\"\n","IMAGE4 = \"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/1200px-Cat03.jpg\"\n","\n","# Downloads the images from their URLs\n","sImageURLs = [IMAGE1, IMAGE2, IMAGE3, IMAGE4]\n","sImageLabels = [\"husky\", \"greek harehound\", \"swiss mountain dog\", \"cat\"]\n","nImageBatch  = np.zeros((len(sImageURLs), 32, 32, 3), np.uint8)\n","for nIndex,sImageURL in enumerate(sImageURLs):\n","  oResponse = requests.get(sImageURL)\n","  nImage = Image.open(BytesIO(oResponse.content))\n","  nImage = nImage.resize((32,32), Image.ANTIALIAS)\n","  nImageIn = np.array(nImage)\n","  nImageBatch[nIndex,:,:,:] = nImageIn[:,:,:]\n","\n","# Z-score standardization for bots sets\n","oDict = joblib.load(r\"/content/gdrive/My Drive/Colab Notebooks/OOT2019/data/tfcifar10/meanstd.pkl\")\n","nPixelMean = oDict[\"mean\"]\n","nPixelStd = oDict[\"std\"]\n","nImageBatchStandardized = (nImageBatch.astype(np.float32) - nPixelMean) / nPixelStd\n","\n","# Initializes the session and its variables\n","print(\"\\n[>] Initializing...\")\n","oSession = tf.Session()\n","oSession.run(tf.global_variables_initializer())\n","\n","# Creates helper objects\n","oState = ModelState(oSession, MODEL_STATE_NAME) \n","oClassNamesDict = DataSetCifar10(DataSubSet, \"/content/gdrive/My Drive/Colab Notebooks/OOT2019/data/cifar10\").ClassNames\n","\n","# Loads the saved state and runs inference\n","if oState.Load():\n","  oImageFeed = oNet.Feed(nImageBatchStandardized, p_bIsTraining=False)\n","  nPredictedProbs, nPredictedClass = oSession.run([oNet.Prediction, oNet.PredictedClass], feed_dict=oImageFeed)\n","  \n","  for nSampleIndex in range(0,nPredictedClass.shape[0]):\n","    sPredictedClass = oClassNamesDict[nPredictedClass[nSampleIndex]]\n","    print(\"\\n[>] The image of a %s is predicted as %s (%d)\" % (sImageLabels[nSampleIndex], sPredictedClass, nPredictedClass[nSampleIndex]))\n","    print(\" |___ Probabilities:%s\" % [\"%s:%.2f%%\" % (oClassNamesDict[nIndex], nPredictedProbs[nSampleIndex, nIndex]*100.0) for nIndex in range(10)])\n","    oImagePlot = plt.imshow(nImageBatch[nSampleIndex,...])\n","    plt.show()\n","  \n","oSession.close()"],"execution_count":0,"outputs":[]}]}