{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"07_OOConvNet.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"xtfmVjeGFDaN","colab_type":"text"},"source":["# Training Deep CNN Image Classifier\n","\n","<img src=\"https://thumbs.gfycat.com/AngryInconsequentialDiplodocus-size_restricted.gif\" width=\"480\" border=\"1\"/>\n","\n","We can use GPU acceleration in Colab from the menu option `Edit -> Notebook Settings`. You can also try your Tensorflow machine learning project on [Tensor Processing Units (TPUs)](https://cloud.google.com/tpu/docs/tpus).\n","\n","\n","(*Objective: Implement training process and inference for a CNN image classifier, preprocessing image data*. Time: 20 mins)"]},{"cell_type":"code","metadata":{"id":"K3Zbk5OeWehu","colab_type":"code","colab":{}},"source":["# Check GPU\n","import tensorflow as tf\n","tf.test.gpu_device_name()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KkvYjmftenNq","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive/')\n","\n","import sys\n","sys.path.append('/content/gdrive/My Drive/Colab Notebooks/OOT2019/lib')\n","\n","# Display the system path elements\n","for sFolder in sys.path:\n","  print(sFolder)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eiDHHTcYDZQC","colab_type":"text"},"source":["## Convolutional Neural Network (CNN)"]},{"cell_type":"markdown","metadata":{"id":"HnQtv4dcecFo","colab_type":"text"},"source":["The base class with common CNN layers is `:ConvolutionalNeuralNetwork` and will included in our framework in th **ootf.cnn**  namespace \n","\n","### Implementation Notes\n","* The descendant class `:SimpleCNN` creates a custom network overriding the virtual method `CreateModel()`. It also adds additional methods for our training process. \n","* The `:ModuleState` is a helper class to save/load the model parameters to/from a file.\n","*   The private method `__defineCostFunction()` implements L2 regularization by adding a term to the categorial cross entropy loss.\n","\n","\n","### Theory: The ELU activation function:\n","*   In this model we are using the [ELU](https://arxiv.org/abs/1511.07289) activation. The ReLU are non-negative so there are no contrastive neurons that try to cancel each other, introducing bias in the subsequent layer.\n","\n","     <img src=\"https://cdn.tinymind.com/static/img/learn/elu.png\" width=\"400\" border=\"1\"/>"]},{"cell_type":"code","metadata":{"id":"lJzmFP_-MPmZ","colab_type":"code","colab":{}},"source":["import os\n","from datetime import datetime\n","import numpy as np\n","import tensorflow as tf\n","from ootf.nn import NeuralNetwork\n","\n","RANDOM_SEED = 2019\n","\n","# Make random initialization reproducible\n","tf.reset_default_graph() \n","tf.set_random_seed(RANDOM_SEED)\n","np.random.seed(RANDOM_SEED)\n","\n"," \n","#==================================================================================================  \n","class ConvolutionalNeuralNetwork(NeuralNetwork):  \n","    #------------------------------------------------------------------------------------\n","    def __init__(self, p_nFeatures=[128,256,512,10]):\n","        #........ |  Instance Attributes | ..............................................\n","        # // Composite object collections \\\\\n","        # ... Weights ...\n","        self.ConvWeights        = []\n","        self.ConvBiases         = []\n","        # ... Layers ...\n","        self.MaxPoolLayers      = []\n","        self.AveragePoolLayers  = []\n","        \n","        # // Architectural Hyperparameters \\\\\n","        self.Features = p_nFeatures\n","        #................................................................................\n","        \n","        # Invoke the inherited logic from ancestor :NeuralNetwork\n","        super(ConvolutionalNeuralNetwork, self).__init__()\n","    #------------------------------------------------------------------------------------\n","    def Convolutional(self, p_tInput, p_nNeuronCount, p_nWindowSize=[3,3], p_nStrides=[1,1], p_bIsPadding=True, p_nPadding=None, p_bHasBias=True, p_tActivationFunction=None):\n","      nLayerNum = len(self.ConvWeights) + 1\n","      \n","      nInputShape  = p_tInput.get_shape().as_list()\n","      nInFeatures  = nInputShape[3]          \n","      nKernelShape = list(p_nWindowSize) + [nInFeatures, p_nNeuronCount]\n","      nStrides     = [1] + list(p_nStrides) + [1]\n","      tX = p_tInput\n","      if p_nPadding is not None:\n","        sPadding = \"VALID\"\n","        tX = self.PadSpatial(p_tInput, p_nWindowSize[0], p_nPadding)\n","      else:      \n","        if p_bIsPadding:\n","          sPadding = \"SAME\"\n","        else:\n","          sPadding = \"VALID\"\n","      \n","      sLayerName = \"CONV%d\" % nLayerNum\n","      with tf.variable_scope(sLayerName):\n","        tW = self.GetParameter(nKernelShape, tf.initializers.he_uniform()) #He weight initilization\n","        tU = tf.nn.conv2d(tX, tW, strides=nStrides, padding=sPadding) \n","        \n","        if p_bHasBias:\n","          tB = self.GetParameter([p_nNeuronCount], p_bIsBias=True)\n","          tU = tU + tB\n","        \n","        if p_tActivationFunction is not None:\n","          # Activation function is a function reference that is passed to the method\n","          tA = p_tActivationFunction(tU)\n","        else:\n","          tA = tU\n","      \n","        self.ConvWeights.append(tW)\n","        if p_bHasBias:\n","          self.ConvBiases.append(tB)\n","        \n","      print(\"    [%s] Input:%s, Kernel:%s, Output:%s\" % (sLayerName, nInputShape, nKernelShape, tA.get_shape().as_list()))\n","      \n","      return tA\n","    #------------------------------------------------------------------------------------\n","    def MaxPool(self, p_tInput, p_nPoolSize=[2,2], p_nPoolStrides=[2,2], p_bIsPadding=True, p_nPadding=None):\n","      nLayerNum = len(self.MaxPoolLayers) + 1\n","      nInputShape  = p_tInput.get_shape().as_list()\n","\n","      tX = p_tInput\n","      if p_nPadding is not None:\n","        sPadding = \"VALID\"\n","        tX = self.PadSpatial(p_tInput, p_nPoolSize[0], p_nPadding)\n","      else:      \n","        if p_bIsPadding:\n","          sPadding = \"SAME\"\n","        else:\n","          sPadding = \"VALID\"\n","        \n","      nSize = [1] + list(p_nPoolSize) + [1]\n","      nStrides = [1] + list(p_nPoolStrides) + [1]\n","       \n","      sLayerName = \"MAXP%d\" % nLayerNum\n","      with tf.variable_scope(sLayerName):\n","        tA = tf.nn.max_pool(tX, ksize=nSize, strides=nStrides, padding=sPadding)\n","        self.MaxPoolLayers.append(tA)\n","      \n","      print(\"    [%s] Input:%s, Pool:%dx%d/%s, Output:%s\" % (sLayerName, \n","                  nInputShape, p_nPoolSize[0], p_nPoolSize[1], p_nPoolStrides[0], tA.get_shape().as_list()))\n","      \n","      return tA \n","    # --------------------------------------------------------------------------------------------------------\n","    def AveragePool(self, p_tInput, p_nPoolSize=(2,2), p_nPoolStrides=(2,2), p_bIsPadding=False):\n","      nLayerNum = len(self.AveragePoolLayers) + 1\n","      nInputShape  = p_tInput.get_shape().as_list()\n","\n","      if p_bIsPadding:\n","        sPadding = \"SAME\"\n","      else:\n","        sPadding = \"VALID\"\n","        \n","      nSize = [1] + list(p_nPoolSize) + [1]\n","      nStrides = [1] + list(p_nPoolStrides) + [1]\n","      \n","      sLayerName = \"AVGP%d\" % nLayerNum\n","      with tf.variable_scope(sLayerName):         \n","        tA = tf.nn.avg_pool(p_tInput, ksize=nSize, strides=nStrides, padding=sPadding)\n","        self.AveragePoolLayers.append(tA)\n","        \n","      print(\"    [%s] Input:%s, Pool:%dx%d/%s, Output:%s\" % (sLayerName, \n","                  nInputShape, p_nPoolSize[0], p_nPoolSize[1], p_nPoolStrides[0], tA.get_shape().as_list()))\n","             \n","      return tA    \n","    # --------------------------------------------------------------------------------------------------------\n","    def Flatten(self, x):\n","        nFlatConvDims = np.prod(np.asarray(x.get_shape().as_list()[1:]))\n","        tFlatten = tf.reshape(x, [-1, nFlatConvDims])\n","        return tFlatten      \n","    # --------------------------------------------------------------------------------------------------------\n","    def GlobalAveragePooling(self, p_tInput):\n","      nInputShape  = p_tInput.get_shape().as_list()\n","      nPoolSize = [1,nInputShape[1],nInputShape[2],1]\n","      \n","      with tf.variable_scope(\"GAVGPOOL\"):\n","        tA = tf.nn.avg_pool(p_tInput, ksize=nPoolSize, strides=[1,1,1,1], padding=\"VALID\", name=\"avgpool\")\n","        tA = tf.squeeze(tA, [1,2], name=\"squeeze\")\n","        \n","      print(\"    [GAVGPOOL] Input:%s, Output:%s\" % (nInputShape, tA.get_shape().as_list()))\n","      \n","      return tA\n","    # --------------------------------------------------------------------------------------------------------\n","    \n","#==================================================================================================\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","# =======================================================================================================================\n","class ModelState(object):\n","    #------------------------------------------------------------------------------------\n","    def __init__(self, p_oSession, p_sFileName):\n","      self.Session  = p_oSession\n","      self.FileName = p_sFileName\n","      oVarList     = tf.global_variables()\n","      self.Saver   = tf.train.Saver(oVarList, write_version=2)\n","    #------------------------------------------------------------------------------------            \n","    def Save(self):\n","        print(\"  |__ Saving Weights to \" + self.FileName, end=\"\")\n","        \n","        self.Saver.save(self.Session, self.FileName)\n","        print(\" -> Saved.\")\n","    #------------------------------------------------------------------------------------\n","    def Load(self):\n","      if os.path.exists(self.FileName + \".meta\"):\n","        print(\"  |__ Restoring Weights from \" + self.FileName, end=\"\")\n","        self.Saver.restore(self.Session, self.FileName)\n","        print(' -> Restored.')\n","        bResult = True\n","      else:\n","        bResult = False\n","        \n","      return bResult\n","    # --------------------------------------------------------------------------------------------------------            \n","    \n","# =======================================================================================================================\n","\n","\n","\n","\n","\n","    \n","#==================================================================================================  \n","class SimpleCNN(ConvolutionalNeuralNetwork):  \n","    #------------------------------------------------------------------------------------\n","    def __init__(self, p_nFeatures=[16,24,32,48,10]):\n","        #........ |  Instance Attributes | ..............................................\n","        # // Tensors \\\\\n","        self.DropOutKeepProb = None\n","        self.CCECost        = None\n","        self.L2Cost         = None\n","        self.CostFunction   = None\n","        self.LearningRate   = None\n","        self.TrainingOp     = None\n","        \n","        # // Learning Hyperparameters \\\\\n","        self.Momentum     = 0.9\n","        self.WeightDecay  = 1e-4\n","        self.DropOutRate   = 0.5\n","        #................................................................................\n","        \n","        # Invoke the inherited logic from ancestor :NeuralNetwork\n","        super(SimpleCNN, self).__init__(p_nFeatures)\n","    #------------------------------------------------------------------------------------\n","    def Feed(self, p_nBatchFeatures, p_nBatchTargets=None, p_nLearningRate=None, p_bIsTraining=False):\n","      oDict = dict()\n","      oDict[self.Input]        = p_nBatchFeatures\n","      \n","      if p_bIsTraining:\n","        oDict[self.LearningRate] = p_nLearningRate\n","        oDict[self.DropOutKeepProb] = self.DropOutRate\n","        oDict[self.Targets]      = p_nBatchTargets\n","      else:\n","        # When the model is trained the neurons are not dropped out\n","        oDict[self.DropOutKeepProb] = 1.0\n","      \n","      return oDict\n","    # --------------------------------------------------------------------------------------------------------\n","    def Predict(self, p_oSession, p_oSubSet):   \n","      nPredictedClasses = np.zeros(p_oSubSet.Labels.shape, np.uint32)\n","      for nRange, nSamples, nLabels, _ in p_oSubSet:\n","        nPrediction = p_oSession.run(self.Prediction, feed_dict=self.Feed(nSamples, nLabels))\n","        nPredictedClasses[nRange] = np.argmax(nPrediction, axis=1).astype(np.uint32)\n","      \n","      return nPredictedClasses        \n","    #------------------------------------------------------------------------------------\n","    def CreateModel(self):\n","      with tf.variable_scope(\"HyperParams\"):\n","        self.DropOutKeepProb = tf.placeholder(tf.float32, shape=())\n","        \n","      with tf.variable_scope(\"NeuralNet\"):\n","        with tf.variable_scope(\"Input\"):\n","          # None in the first dimension means variable batch size\n","          self.Input = tf.placeholder(tf.float32, shape=(None,32,32,3))\n","          self.Targets = tf.placeholder(tf.uint8, shape=(None))\n","\n","        # First convolutional layer performs downsampling\n","        tA = self.Convolutional(self.Input, self.Features[0], [3,3], [1,1], p_tActivationFunction=tf.nn.elu)\n","        \n","        # Declare 3 modules of 2 convolutions each, followed by a max pooling operation and local response normalization\n","        for nModuleIndex in range(0, 3):\n","          sModuleName = \"Module%d\" % (nModuleIndex + 1)\n","          \n","          with tf.variable_scope(sModuleName):\n","            print(\" |_ \" + sModuleName)\n","\n","            # Downsample only at module 2 and 3 \n","            if nModuleIndex > 0:\n","              tA = self.MaxPool(tA, [3,3], [2,2])\n","            \n","            for _ in range(0, 2):\n","              tA = self.Convolutional(tA, self.Features[1 + nModuleIndex], [3,3], [1,1], p_tActivationFunction=tf.nn.elu)\n","              \n","            tA = tf.nn.lrn(tA)\n","        \n","        # The 3D activation volume of a sample is flattened into a vector\n","        tA = self.Flatten(tA)\n","        \n","        # Dropout layer to prevent overfitting in the classification layer\n","        tA = self.DropOut(tA)\n","        \n","        self.Logits     = self.FullyConnected(tA, self.Features[-1])\n","        self.Prediction = tf.nn.softmax(self.Logits)\n","        \n","        self.PredictedClass = tf.argmax(self.Prediction, axis=1)\n","        \n","      self.__defineCostFunction()\n","    # --------------------------------------------------------------------------------------------------------\n","    def __defineCostFunction(self):\n","        nClassCount = self.Features[-1]\n","      \n","        with tf.variable_scope(\"Cost\"):\n","            # Multiclass categorical cross entropy (CCE) loss\n","            tLoss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.Logits, labels=tf.one_hot(self.Targets, depth=nClassCount, dtype=tf.float32))\n","            self.CCECost  = tf.reduce_mean(tLoss, name=\"cce\")\n","            \n","            # L2 weight decay regularizer\n","            tL2LossesConv = [tf.nn.l2_loss(tKernel) for tKernel in self.ConvWeights]\n","            tL2LossesFC   = [tf.nn.l2_loss(tWeight) for tWeight in self.FCWeights]\n","            tL2LossesAll  = tL2LossesConv + tL2LossesFC\n","            \n","            self.L2Cost = tf.multiply(tf.constant(self.WeightDecay, tf.float32), tf.add_n(tL2LossesAll), name=\"l2\")\n","\n","            self.CostFunction = tf.identity(self.CCECost + self.L2Cost, \"total_cost\")\n","    # --------------------------------------------------------------------------------------------------------\n","#==================================================================================================  \n","\n","  \n","  \n","  \n","\n","\n","\n","oNet = SimpleCNN()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XRwc0qymQI1h","colab_type":"text"},"source":["## Loading and preprocessing the data\n","We load the whole dataset into memory and standardize the RGB pixel values.\n","\n","### Theory: Z-Score standardization\n","\n","\n","<img src=\"http://www.z-table.com/uploads/2/1/7/9/21795380/5175170_orig.gif\" width=\"150\"/>"]},{"cell_type":"code","metadata":{"id":"edjkhN0gQB7V","colab_type":"code","colab":{}},"source":["import joblib\n","from ootf.base import DataSubSet\n","from ootf.cifar10 import DataSetCifar10\n","\n","# Loads the dataset\n","oData = DataSetCifar10(DataSubSet, \"/content/gdrive/My Drive/Colab Notebooks/OOT2019/data/cifar10\")\n","oData.Load()\n","oData.Training.BatchSize=500\n","oData.Testing.BatchSize=1000\n","\n","# Z-score standardization for bots sets\n","# ... We have saved the mean and std for CIFAR10 training subset ...\n","oDict = joblib.load(r\"/content/gdrive/My Drive/Colab Notebooks/OOT2019/data/tfcifar10/meanstd.pkl\")\n","nPixelMean = oDict[\"mean\"]\n","nPixelStd = oDict[\"std\"]\n","oData.Training.Samples = (oData.Training.Samples - nPixelMean) / nPixelStd\n","oData.Testing.Samples = (oData.Testing.Samples - nPixelMean) / nPixelStd\n","\n","\n","np.set_printoptions(suppress=True)\n","print(\"Training subset mean R,G,B:%s \" % np.round(np.mean(oData.Training.Samples, axis=(0,1,2)), 6))\n","print(\"Testing subset mean R,G,B:%s \" % np.round(np.mean(oData.Testing.Samples, axis=(0,1,2)), 6))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4W8Yp_S6DLPu","colab_type":"text"},"source":["# Training\n","![Supervised Learning](https://www.cmu.edu/roboticsacademy/images/Sundry%20Images/1_OnsiteIcon.jpg)"]},{"cell_type":"markdown","metadata":{"id":"B178ZvUueaSQ","colab_type":"text"},"source":["## Initializing optimizer, training operation and session\n","\n","We are going to use the [:ADAMOptimizer](https://arxiv.org/abs/1412.6980) class, that implements the respective gradient descent method. The method uses exponential moving averages of gradients for adaptive learning. The gradients and the backpropagation operations are created by the `minimize()` method"]},{"cell_type":"code","metadata":{"id":"rt43_XhfGcLR","colab_type":"code","colab":{}},"source":["# Creates the training operation automatically\n","tOptimizer = tf.train.AdamOptimizer(oNet.LearningRate)\n","tTrainOp = tOptimizer.minimize(oNet.CostFunction)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VIQtv124My6A","colab_type":"code","colab":{}},"source":["# Initializes the session and its variables\n","print(\"\\n[>] Initializing...\")\n","oSession = tf.Session()\n","oSession.run(tf.global_variables_initializer())\n","\n","# Creates helper objects\n","oState = ModelState(oSession, \"/content/gdrive/My Drive/Colab Notebooks/OOT2019/models/simplecnn\")              \n","  \n","print(\" |__ GPU:\", tf.test.gpu_device_name())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kc_KQ-P2MzEI","colab_type":"text"},"source":["## Main training loop\n","\n","It requires about 7 minutes for 50 epochs to train the model with ~81.7% accuracy. An epoch is completed when all samples in the training subset are presented to the model."]},{"cell_type":"code","metadata":{"id":"GL35PRrweZcv","colab_type":"code","colab":{}},"source":["from sklearn.metrics import accuracy_score\n","\n","print(\"[>] Started training at %s ...\" % datetime.now() )\n","      \n","\n","# Training loop   \n","nMaxEpochs = 50\n","nLR = 0.005 \n","\n","nEpochNumber = 1\n","nStepNumber = 1\n","while nEpochNumber <= nMaxEpochs:\n","\n","  nPredictedClasses = np.zeros(oData.Training.Labels.shape, np.uint32)\n","  for nRange, nSamples, nLabels, _ in oData.Training:\n","    oDict = oNet.Feed(nSamples, nLabels, nLR, p_bIsTraining=True)\n","    oTensorList = [tTrainOp, oNet.CostFunction, oNet.CCECost, oNet.L2Cost, oNet.Prediction, oNet.PredictedClass]\n","    _, nTrainingTotalLoss, nTrainingCCELoss, nTrainingL2Loss, nPrediction, nPredictedClasses[nRange]  = oSession.run(oTensorList, feed_dict=oDict)\n","    #nPredictedClasses[nRange] = np.argmax(nPrediction, axis=1).astype(np.uint32)\n","\n","    if nStepNumber % 100 == 0:\n","      print('%s [%d] Iteration %6d | LR=%.6f | ERR=%.6f (CCE=%.6f) (WD=%.6F)' % (datetime.now(), nEpochNumber, nStepNumber, nLR\n","                                              , nTrainingTotalLoss, nTrainingCCELoss, nTrainingL2Loss))\n","\n","    nStepNumber += 1\n","\n","    # Learning rate schedule  \n","    if (nStepNumber == 2500) or (nStepNumber == 3500):\n","      nLR = nLR / 10.0\n","\n","  # Calculate the accuracy metric for the training subset\n","  nTrainAccuracy = accuracy_score(oData.Training.Labels, nPredictedClasses)\n","\n","  # Evaluation\n","  if ((nEpochNumber % 4) == 0) or (nEpochNumber == nMaxEpochs):\n","    nPredictedClasses = oNet.Predict(oSession, oData.Testing)\n","    # Calculate the accuracy metric for the testing subset\n","    nTestAccuracy = accuracy_score(oData.Testing.Labels, nPredictedClasses)\n","    print(\"[Epoch:%d] Accuracy: Testing=%.2f%% Training=%.2f%% \" % (nEpochNumber, nTestAccuracy*100.0, nTrainAccuracy*100.0))\n","\n","\n","  nEpochNumber += 1\n","\n","# Save trained network weights \n","oState.Save()\n","\n","oSession.close()\n","print(\"[>] Finished training at %s ...\" % datetime.now())  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n5UggOAPCovQ","colab_type":"text"},"source":["# Predicting with a pre-trained model\n","\n","<img src=\"https://miro.medium.com/max/3840/1*oB3S5yHHhvougJkPXuc8og.gif\" width=\"480\" border=\"1\"/>\n","\n","\n","## Restoring a saved state\n","After the model tensors are created in the current graph, a session is initialized and the model parameters are loaded from the saved checkpoint\n","\n","`/content/gdrive/My Drive/Colab Notebooks/OOT2019/models/simplecnn-data-00000-of-00001`\n"]},{"cell_type":"code","metadata":{"id":"CVyTFAuSByXk","colab_type":"code","colab":{}},"source":["# Initializes the session and its variables\n","print(\"\\n[>] Initializing...\")\n","oSession = tf.Session()\n","oSession.run(tf.global_variables_initializer())\n","\n","# Creates helper objects\n","oState = ModelState(oSession, \"/content/gdrive/My Drive/Colab Notebooks/OOT2019/models/simplecnn\") \n","\n","if oState.Load():\n","  nPredictedClasses = oNet.Predict(oSession, oData.Testing)\n","  # Calculate the accuracy metric for the testing subset\n","  nTestAccuracy = accuracy_score(oData.Testing.Labels, nPredictedClasses)\n","  print(\"Trained model accuracy:%.2f%%\" % (nTestAccuracy*100.0))  \n","  \n","  \n","oSession.close()  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jm-lY4A5FzFn","colab_type":"text"},"source":["## Neural network inference\n","The neural network inference is to predict the class given an image. We are going to test our network by recalling images from the web, that will be resized to 32 x 32 in order to fit the pre-trained network input size. This crude downsampling results in loss of visual information that is critical to correctly classify the image."]},{"cell_type":"code","metadata":{"id":"j7rVdbGRFxH-","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import requests\n","from PIL import Image\n","from io import BytesIO\n","\n","# ... False predictions ...\n","IMAGE1 = \"https://upload.wikimedia.org/wikipedia/commons/thumb/8/8b/Husky_on_San_Francisco_sidewalk.jpg/220px-Husky_on_San_Francisco_sidewalk.jpg\"\n","IMAGE2 = \"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b1/Hellenic_Hound_aka_Hellinikos_Ichnilatis.jpg/220px-Hellenic_Hound_aka_Hellinikos_Ichnilatis.jpg\"\n","\n","# ... Correct predictions ...\n","IMAGE3 = \"https://vignette.wikia.nocookie.net/dog-breed4080/images/0/0a/GREATER-SWISS-MOUNTAIN-DOG.jpg/revision/latest/scale-to-width-down/230?cb=20171221134620\"\n","IMAGE4 = \"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/1200px-Cat03.jpg\"\n","\n","# Downloads the images from their URLs\n","sImageURLs = [IMAGE1, IMAGE2, IMAGE3, IMAGE4]\n","sImageLabels = [\"husky\", \"greek harehound\", \"swiss mountain dog\", \"cat\"]\n","nImageBatch  = np.zeros((len(sImageURLs), 32, 32, 3), np.uint8)\n","for nIndex,sImageURL in enumerate(sImageURLs):\n","  print(sImageURL)\n","  oResponse = requests.get(sImageURL)\n","  nImage = Image.open(BytesIO(oResponse.content))\n","  nImage = nImage.resize((32,32), Image.ANTIALIAS)\n","\n","  oImagePlot = plt.imshow(nImage)\n","  plt.show()\n","  nImageIn = np.array(nImage)\n","\n","  nImageBatch[nIndex,:,:,:] = nImageIn[:,:,:]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MeG4RhWiZTcB","colab_type":"text"},"source":["We need to z-score standardize the image, because our model `y^=f(z;θ)` has been trained using transformed image representations `z = Φ(x)` of the original image representations `x`"]},{"cell_type":"code","metadata":{"id":"TwOt-_O5GImk","colab_type":"code","colab":{}},"source":["import joblib\n","from ootf.base import DataSubSet\n","from ootf.cifar10 import DataSetCifar10\n","\n","MODEL_STATE_NAME = \"/content/gdrive/My Drive/Colab Notebooks/OOT2019/models/simplecnn\" \n","\n","\n","# Z-score standardization for bots sets\n","# ... We have saved the mean and std for CIFAR10 training subset ...\n","oDict = joblib.load(r\"/content/gdrive/My Drive/Colab Notebooks/OOT2019/data/tfcifar10/meanstd.pkl\")\n","nPixelMean = oDict[\"mean\"]\n","nPixelStd = oDict[\"std\"]\n","\n","nImageBatchStandardized = (nImageBatch.astype(np.float32) - nPixelMean) / nPixelStd\n","\n","# Initializes the session and its variables\n","print(\"\\n[>] Initializing...\")\n","oSession = tf.Session()\n","oSession.run(tf.global_variables_initializer())\n","\n","# Creates helper objects\n","oState = ModelState(oSession, MODEL_STATE_NAME) \n","oClassNamesDict = DataSetCifar10(DataSubSet, \"/content/gdrive/My Drive/Colab Notebooks/OOT2019/data/cifar10\").ClassNames\n","\n","# Loads the saved state and runs inference\n","if oState.Load():\n","  oImageFeed = oNet.Feed(nImageBatchStandardized, p_bIsTraining=False)\n","  nPredictedProbs, nPredictedClass = oSession.run([oNet.Prediction, oNet.PredictedClass], feed_dict=oImageFeed)\n","  \n","  for nSampleIndex in range(0,nPredictedClass.shape[0]):\n","    sPredictedClass = oClassNamesDict[nPredictedClass[nSampleIndex]]\n","    \n","    print(\"\\n[>] The image of a %s is predicted as %s (%d)\" % (sImageLabels[nSampleIndex], sPredictedClass, nPredictedClass[nSampleIndex]))\n","    print(\" |___ Probabilities:%s\" % [\"%s:%.2f%%\" % (oClassNamesDict[nIndex], nPredictedProbs[nSampleIndex, nIndex]*100.0) for nIndex in range(10)])\n","  \n","  \n","oSession.close() "],"execution_count":0,"outputs":[]}]}